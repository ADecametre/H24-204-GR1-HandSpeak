{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADecametre/H24-204-GR1-HandSpeak/blob/Entra%C3%AEnementMod%C3%A8le/Entrainement_du_modele_HandSpeak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Personnalisation du modèle HandSpeak pour la reconnaissance de la langue des signes américaine.\n",
        "Source : [Hand gesture recognition model customization guide  |  MediaPipe  |  Google for Developers](https://developers.google.com/mediapipe/solutions/customization/gesture_recognizer)\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      Voir le code original sur GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "Le package MediaPipe Model Maker est une solution à faible code pour personnaliser les modèles d'apprentissage automatique (ML).\n",
        "\n",
        "\n",
        "Ce notebook nous permet de:\n",
        "1.   Créer des données grâce à la webcam.\n",
        "2.   Entrainer un modèle avec ces données.\n",
        "3.   Tester et améliorer la performance du modèle.\n",
        "4.   Télécharger le modèle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Préalables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Installer le package MediaPipe Model Maker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install 'keras<3.0.0' mediapipe-model-maker\n",
        "!pip install simplejson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Importer les librairies nécessaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Création du modèle\n",
        "\n",
        "Les étapes suivantes sont celles qu'on a entrepris pour créer notre modèle HandSpeak qui détecte le langage des signes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Capture et structuration des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "Le format requis pour l'ensemble de données de reconnaissance de gestes dans Model Maker est le suivant : <chemin_de_ensemble_de_données>/<nom_d'étiquette>/<nom_image>.*. De plus, l'un des noms d'étiquettes (label_names) doit être none. L'étiquette none représente tout geste qui n'est pas classifié comme l'un des autres gestes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dwmyg5MnR_y"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"data\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Capture d'images par webcam\n",
        "# @markdown <small>Si la webcam ne n'affiche pas, redémarrer le code.</small>\n",
        "# @markdown <ul>\n",
        "# @markdown <li>Le code utilise du JavaScript pour interagir avec la webcam et capturer des images. Il crée une interface utilisateur avec des boutons pour démarrer la capture, arrêter la capture et afficher la caméra en temps réel.</li>\n",
        "# @markdown <li>Une fois que la capture est lancée, l'utilisateur peut entrer le nom de l'étiquette pour chaque signe à capturer.</li>\n",
        "# @markdown <ul><li>Le code gère la création de nouveaux dossiers pour chaque étiquette si nécessaire, ainsi que la suppression du contenu des dossiers existants si l'utilisateur le souhaite.</li></ul></li>\n",
        "# @markdown <li>Ensuite, l'utilisateur peut capturer des photos en cliquant sur le bouton \"Capture\". Les images capturées sont enregistrées dans des dossiers correspondant aux étiquettes.</li>\n",
        "# @markdown <li>Lorsque l'utilisateur clique sur le bouton \"Stop\", le code demande l'étiquette du signe suivant pour lequelle on veut capturer des phtotos.</li>\n",
        "# @markdown <li>Si l'utilisateur décide de ne pas ajouter une étiquette et clique sur le bouton \"Annuler\", alors le code s'arrête.</li>\n",
        "# @markdown </ul>\n",
        "\n",
        "from IPython.display import display, Javascript, JSON\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import time\n",
        "import simplejson as json\n",
        "import errno\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def capture(quality=0.8):\n",
        "\n",
        "  # CODE JAVASCRIPT\n",
        "\n",
        "  js = Javascript('''\n",
        "\n",
        "    // Initialise, puis affiche la webcam en tant réel\n",
        "    async function initialiserWebcam(){\n",
        "      // Création du HTML (div et boutons)\n",
        "      document.body.innerHTML = \"\";\n",
        "      const div = document.createElement('div');\n",
        "      div.setAttribute(\"id\", \"divCapture\");\n",
        "      const capture = document.createElement('button');\n",
        "      capture.setAttribute(\"id\", \"boutonCapture\");\n",
        "      const stop = document.createElement('button');\n",
        "      stop.setAttribute(\"id\", \"boutonArretCapture\");\n",
        "      capture.textContent = 'Capture';\n",
        "      stop.textContent = 'Stop';\n",
        "      div.appendChild(capture);\n",
        "      div.appendChild(stop);\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      // Connexion à la webcam\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      // Affichage vidéo\n",
        "      const video = document.createElement('video');\n",
        "      video.setAttribute(\"id\", \"webcam\");\n",
        "      video.style.display = 'block';\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "    }\n",
        "\n",
        "    // Capture la webcam et retourne l'image\n",
        "    async function prendrePhoto(quality){\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      return await new Promise(resolve => {\n",
        "\n",
        "        // Si le bouton Capture est cliqué\n",
        "        capture = document.getElementById(\"boutonCapture\");\n",
        "        capture.onclick = ()=>{\n",
        "          // Dessine la webcam sur un canvas\n",
        "          const canvas = document.createElement('canvas');\n",
        "          video = document.getElementById(\"webcam\");\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "          // Retourne le canvas sous forme d'image\n",
        "          resolve(canvas.toDataURL('image/jpeg', quality))\n",
        "        };\n",
        "\n",
        "        // Si le bouton Stop est cliqué\n",
        "        stop = document.getElementById(\"boutonArretCapture\");\n",
        "        stop.onclick = ()=>{\n",
        "          // Retourne null\n",
        "          resolve(null);\n",
        "        };\n",
        "\n",
        "      });\n",
        "    }\n",
        "\n",
        "    // Désactive et met fin à l'affichage de la webcam\n",
        "    async function arreterCapture(){\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      stream.getVideoTracks().forEach(track => track.stop);\n",
        "      div = document.getElementById(\"divCapture\");\n",
        "      div.remove();\n",
        "    }\n",
        "\n",
        "  ''').data\n",
        "\n",
        "  # Demande le nom du signe qui sera capturé\n",
        "  def demander_label():\n",
        "    return eval_js('prompt(\"Veuillez entrer le nom du label (ex. : A).\")')\n",
        "\n",
        "  # Exécute prendrePhoto()\n",
        "  def prendre_photo():\n",
        "    return eval_js('prendrePhoto({})'.format(quality))\n",
        "\n",
        "  # Confirme la suppression ou non d'un dossier\n",
        "  def confirmation_suppression(path):\n",
        "    js_confirm = f'confirm(\"Voulez-vous supprimer le contenu du dossier {path} ?\")'\n",
        "    js_confirm_2 = f'confirm(\"Êtes-vous sûr de vouloir supprimer le contenu du dossier {path} ? Cette action est irréversible.\")'\n",
        "    return eval_js(js_confirm) and eval_js(js_confirm_2)\n",
        "\n",
        "\n",
        "  # Exécute le code JavaScript\n",
        "  eval_js(js)\n",
        "  eval_js('initialiserWebcam()')\n",
        "\n",
        "  # Demande le nom du signe à capturer (tant qu'il y en a)\n",
        "  label = demander_label()\n",
        "  while(label != None):\n",
        "    path = f'{dataset_path}/{label}'\n",
        "    try:\n",
        "      # Crée un dossier avec le nom du signe\n",
        "      os.makedirs(path)\n",
        "    except OSError as exc:\n",
        "      if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "        # Si le dossier existe déjà, confirme la suppression ou non du dossier\n",
        "        if(confirmation_suppression(path)):\n",
        "          shutil.rmtree(path)\n",
        "          os.makedirs(path)\n",
        "        pass\n",
        "      else: raise\n",
        "\n",
        "    # Prend la photo (tant qu'il y en a) puis l'enregistre dans le dossier\n",
        "    data = prendre_photo()\n",
        "    while(data != None):\n",
        "      binary = b64decode(data.split(',')[1])\n",
        "      with open(f'{path}/{time.time()}.jpg', 'wb+') as f:\n",
        "        f.write(binary)\n",
        "      data = prendre_photo()\n",
        "\n",
        "    label = demander_label()\n",
        "\n",
        "  # Arrête la capture lorsqu'il n'y a plus de signe à capturer\n",
        "  eval_js('arreterCapture()')\n",
        "  return\n",
        "\n",
        "try:\n",
        "  capture()\n",
        "except Exception as err:\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "hs57kXNZd0_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Vérifier que l'on a tout les catégories de signe de main capturées avec la webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "!rm -rf `find -type d -name .ipynb_checkpoints`\n",
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Entraînement de l'IA\n",
        "L'entraînement de l'IA va se faire sur 4 étapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "#### Charger le dataset\n",
        "\n",
        "Chargez le dataset situé à `chemin_du_dataset` en utilisant la méthode `Dataset.from_folder`. Lors du chargement du dataset, on exécute le modèle de détection de main pré-emballé de MediaPipe Hands pour détecter les repères de la main à partir des images. Toutes les images sans mains détectées sont omises du dataset. Le dataset résultant contiendra les positions des repères de la main extraites de chaque image, plutôt que les images elles-mêmes.\n",
        "\n",
        "La classe `HandDataPreprocessingParams` contient deux options configurables pour le processus de chargement des données :\n",
        "\n",
        "* `shuffle` : Un booléen contrôlant s'il faut mélanger le jeu de données. Par défaut, il est défini sur vrai.\n",
        "* `min_detection_confidence` : Un float compris entre 0 et 1 contrôlant le seuil de confiance pour la détection de main.\n",
        "\n",
        "On divise le dataset : 80% pour l'entraînement, 10% pour la validation et 10% pour les tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "# @title Paramètres de chargement du dataset\n",
        "\n",
        "# @markdown #### `HandDataPreprocessingParams`\n",
        "shuffle = True # @param {type:\"boolean\"}\n",
        "min_detection_confidence = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# @markdown <sup><sub>Valeurs par défaut : True, 0.7 </sup></sub>\n",
        "\n",
        "# @markdown #### `Division des données`\n",
        "data_split = 0.8 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "rest_data_split = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# @markdown <sup><sub>Valeurs par défaut : 0.8, 0.5 </sup></sub>\n",
        "\n",
        "\n",
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams(shuffle=shuffle, min_detection_confidence=min_detection_confidence)\n",
        ")\n",
        "train_data, rest_data = data.split(data_split)\n",
        "validation_data, test_data = rest_data.split(rest_data_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "#### Entraîner le modèle\n",
        "\n",
        " Entraîner le reconnaisseur de gestes personnalisé en utilisant la méthode create et en passant les données d'entraînement, les données de validation, les options du modèle et les hyperparamètres. Pour plus d'informations sur les options du modèle et les hyperparamètres, consultez la section [Hyperparamètres](#scrollTo=F1tiLGGRcvhy) ci-dessus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "#### Évaluer la performance du modèle\n",
        "\n",
        "Après avoir entraîné le modèle, on l'évalue sur un dataset de test et on imprime les métriques de perte et de précision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "#### Exporter à Tensorflow Lite Model\n",
        "\n",
        "Après avoir créé le modèle, on le convertit et on l'exporte au format de modèle Tensorflow Lite pour une utilisation ultérieure dans une application sur appareil. L'exportation comprend également les métadonnées du modèle, qui incluent le fichier d'étiquettes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1UMEG85hQL_"
      },
      "source": [
        "On peut personnaliser davantage le modèle en utilisant la classe `GestureRecognizerOptions`, qui comporte deux paramètres optionnels pour `ModelOptions` et `HParams`. On utilise la classe `ModelOptions` pour personnaliser les paramètres liés au modèle lui-même, et la classe `HParams` pour personnaliser les autres paramètres liés à l'entraînement et à la sauvegarde du modèle.\n",
        "\n",
        "[`ModelOptions`](https://developers.google.com/mediapipe/api/solutions/python/mediapipe_model_maker/gesture_recognizer/ModelOptions) possède un paramètre personnalisable qui affecte la précision :\n",
        "* `dropout_rate` : La fraction des unités d'entrée à supprimer. Utilisé dans la couche de désactivation aléatoire (dropout). Par défaut, 0,05.\n",
        "* `layer_widths` : Une liste des largeurs de couche cachée pour le modèle de gestes. Chaque élément de la liste créera une nouvelle couche cachée avec la largeur spécifiée. Les couches cachées sont séparées avec BatchNorm, Dropout et ReLU. Par défaut, une liste vide (pas de couches cachées).\n",
        "\n",
        "[`HParams`](https://developers.google.com/mediapipe/api/solutions/python/mediapipe_model_maker/gesture_recognizer/HParams) possède la liste suivante de paramètres personnalisables qui affectent la précision du modèle :\n",
        "* `learning_rate` : Le taux d'apprentissage à utiliser pour l'entraînement par descente de gradient. Par défaut, 0,001.\n",
        "* `batch_size` : Taille du lot pour l'entraînement. Par défaut, 2.\n",
        "* `epochs` : Nombre d'itérations d'entraînement sur l'ensemble de données. Par défaut, 10.\n",
        "* `steps_per_epoch` : Un entier facultatif qui indique le nombre de pas d'entraînement par époque. Si non défini, le pipeline d'entraînement calcule le nombre de pas par époque par défaut comme la taille de l'ensemble de données d'entraînement divisée par la taille du lot.\n",
        "* `shuffle` : True si l'ensemble de données est mélangé avant l'entraînement. Par défaut, False.\n",
        "* `lr_decay` : Taux de décroissance du taux d'apprentissage à utiliser pour l'entraînement par descente de gradient. Par défaut, 0,99.\n",
        "* `gamma` : Paramètre gamma pour la perte focal. Par défaut, 2.\n",
        "\n",
        "Paramètre supplémentaire de `HParams` qui n'affecte pas la précision du modèle :\n",
        "* `export_dir` : L'emplacement des fichiers de point de contrôle du modèle et des fichiers de modèle exportés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvVZeSYBLfV"
      },
      "source": [
        "Par exemple, on peut entraîner un nouveau modèle avec un taux de désactivation (dropout_rate) de 0,2 et un taux d'apprentissage (learning_rate) de 0,003."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparamètres\n",
        "\n",
        "# @markdown #### `ModelOptions`\n",
        "dropout_rate = 0.2 # @param {type:\"number\", placeholder:\"0.003\"}\n",
        "layer_widths = [] # @param {type:\"raw\"}\n",
        "\n",
        "# @markdown #### `HParams`\n",
        "learning_rate = 0.003 # @param {type:\"number\"}\n",
        "batch_size = 2 # @param {type:\"integer\"}\n",
        "epochs = 10 # @param {type:\"integer\"}\n",
        "steps_per_epoch = None # @param {type:\"raw\"}\n",
        "shuffle = False # @param {type:\"boolean\"}\n",
        "export_dir = \"exported_model_2\" # @param {type:\"string\"}\n",
        "lr_decay = 0.99 # @param {type:\"number\"}\n",
        "gamma = 2 # @param {type:\"integer\"}\n",
        "\n",
        "hparams = gesture_recognizer.HParams(learning_rate=learning_rate, batch_size=batch_size, epochs=epochs, steps_per_epoch=steps_per_epoch, shuffle=shuffle, export_dir=export_dir, lr_decay=lr_decay, gamma=gamma)\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=dropout_rate, layer_widths=layer_widths)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cekuTJiBbv9"
      },
      "source": [
        "Evaluer le nouveau modèle entrainé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporter le nouveau modèle."
      ],
      "metadata": {
        "id": "5u0J6cdZAtyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OyF8aFz-3kS"
      },
      "outputs": [],
      "source": [
        "model_2.export_model()\n",
        "!ls $export_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKbiPZLd-8lB"
      },
      "outputs": [],
      "source": [
        "files.download(f'{export_dir}/gesture_recognizer.task')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "sWXwEXSXlg7d",
        "OF9ArLQXIu25",
        "yAXWc3bv8hpe",
        "nED7mdIO9YS6",
        "vJLramjy9gvy",
        "F1tiLGGRcvhy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}